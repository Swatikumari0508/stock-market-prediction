{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98fd8afc-a4d8-4568-b41e-bcd2f62d9631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in c:\\users\\kiit\\anaconda3\\swati\\lib\\site-packages (0.18.0.post0)\n",
      "Requirement already satisfied: nltk>=3.8 in c:\\users\\kiit\\anaconda3\\swati\\lib\\site-packages (from textblob) (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\kiit\\anaconda3\\swati\\lib\\site-packages (from nltk>=3.8->textblob) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\kiit\\anaconda3\\swati\\lib\\site-packages (from nltk>=3.8->textblob) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\kiit\\anaconda3\\swati\\lib\\site-packages (from nltk>=3.8->textblob) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\kiit\\anaconda3\\swati\\lib\\site-packages (from nltk>=3.8->textblob) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\kiit\\anaconda3\\swati\\lib\\site-packages (from click->nltk>=3.8->textblob) (0.4.6)\n",
      "Collecting xgboost\n",
      "  Using cached xgboost-2.1.3-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\kiit\\anaconda3\\swati\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\kiit\\anaconda3\\swati\\lib\\site-packages (from xgboost) (1.13.1)\n",
      "Using cached xgboost-2.1.3-py3-none-win_amd64.whl (124.9 MB)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-2.1.3\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Title: Stock market prediction using news headlines\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "!pip install textblob\n",
    "!pip install xgboost\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from textblob import TextBlob\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def analize_sentiment(tweet):\n",
    "    \n",
    "    analysis = TextBlob((str(tweet)))     #defining the function which will find the plority of a sentence\n",
    "    return analysis.polarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be120d36-dc6a-4986-b8d0-3d6ab524ed7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "news= pd.read_csv('Combined_News_DJIA.csv')\n",
    "\n",
    "train_news = news[news['Date'] < '2014-07-15']   # SPLITTING THE DATASET INTO TRAINING AND TESTING\n",
    "test_news = news[news['Date'] > '2014-07-14']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "faad9632-9eef-4124-be8c-fb4b569fc773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE TABLE OF FREQUENCY WORD DISTRIBUTION (1492, 4733)\n"
     ]
    }
   ],
   "source": [
    "train_news_list = []\n",
    "for row in range(0,len(train_news.index)): # CONVERT THE TRAINNG DATASET OF 27 COLUMNS INTO ONE ELEMENT IN THE LIST FOR EACH DAY\n",
    "    train_news_list.append(' '.join(str(k) for k in train_news.iloc[row,2:27]))\n",
    "    \n",
    "vectorize= CountVectorizer(min_df=0.01, max_df=0.8) # DEFINING THE VECTOR FUNCTION, SPECIFYING THR MIN AND MAX WORD FREQUENCY FILTER\n",
    "news_vector = vectorize.fit_transform(train_news_list) # TRANSFORMING THE TRAINING DATASET INTO WORD FREQUENCY TRANFORMATION\n",
    "print( \"THE TABLE OF FREQUENCY WORD DISTRIBUTION\" , news_vector.shape)\n",
    "\n",
    "lr=LogisticRegression()\n",
    "model = lr.fit(news_vector, train_news[\"Label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea141ba2-2f40-4824-8017-ef12d5daf41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the baseline model accuracy 0.4567404426559356\n"
     ]
    }
   ],
   "source": [
    "test_news_list = []\n",
    "for row in range(0,len(test_news.index)):\n",
    "    test_news_list.append(' '.join(str(x) for x in test_news.iloc[row,2:27]))# CONVERT THE TESTING DATASET OF 27 COLUMNS INTO ONE ELEMENT IN THE LIST FOR EACH DAY\n",
    "\n",
    "test_vector = vectorize.transform(test_news_list) # TRANSFORMING THE TESTING DATASET INTO WORD FREQUENCY TRANFORMATION\n",
    "\n",
    "predictions = model.predict(test_vector)\n",
    "\n",
    "pd.crosstab(test_news[\"Label\"], predictions, rownames=[\"Actual\"], colnames=[\"Predicted\"])\n",
    "\n",
    "accuracy1=accuracy_score(test_news['Label'], predictions)\n",
    "print(\"the baseline model accuracy\", accuracy1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ff9a581-f5e8-422f-a404-87ad00ec0ee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>to help</td>\n",
       "      <td>-0.872962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>us and</td>\n",
       "      <td>-0.881678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>accused of</td>\n",
       "      <td>-0.950826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>people are</td>\n",
       "      <td>-0.974572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>around the</td>\n",
       "      <td>-1.011382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>south africa</td>\n",
       "      <td>-1.020035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>after being</td>\n",
       "      <td>-1.092408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>to kill</td>\n",
       "      <td>-1.198554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>the country</td>\n",
       "      <td>-1.224351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>up in</td>\n",
       "      <td>-1.276302</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Word  Coefficient\n",
       "242       to help    -0.872962\n",
       "262        us and    -0.881678\n",
       "4      accused of    -0.950826\n",
       "153    people are    -0.974572\n",
       "20     around the    -1.011382\n",
       "175  south africa    -1.020035\n",
       "6     after being    -1.092408\n",
       "244       to kill    -1.198554\n",
       "188   the country    -1.224351\n",
       "260         up in    -1.276302"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nvectorize = TfidfVectorizer(min_df=0.05, max_df=0.85, ngram_range=(2,2))  # Define nvectorize for bigram\n",
    "news_nvector = nvectorize.fit_transform(train_news_list)\n",
    "\n",
    "nmodel = lr.fit(news_nvector, train_news[\"Label\"])\n",
    "\n",
    "nwords = nvectorize.get_feature_names_out()  # Corrected method name\n",
    "ncoefficients = nmodel.coef_.tolist()[0]\n",
    "ncoeffdf = pd.DataFrame({'Word' : nwords, \n",
    "                        'Coefficient' : ncoefficients})\n",
    "ncoeffdf = ncoeffdf.sort_values(['Coefficient', 'Word'], ascending=[0, 1])\n",
    "ncoeffdf.head(10)\n",
    "ncoeffdf.tail(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97871fd2-e65e-475c-8eee-42b7dedd921e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " TFID TRANSFOMATION DATAFRAME SHAPE (1492, 284)\n",
      " Logistics Regression with Bigram and TFID 0.5311871227364185\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>to help</td>\n",
       "      <td>-0.872962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>us and</td>\n",
       "      <td>-0.881678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>accused of</td>\n",
       "      <td>-0.950826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>people are</td>\n",
       "      <td>-0.974572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>around the</td>\n",
       "      <td>-1.011382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>south africa</td>\n",
       "      <td>-1.020035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>after being</td>\n",
       "      <td>-1.092408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>to kill</td>\n",
       "      <td>-1.198554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>the country</td>\n",
       "      <td>-1.224351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>up in</td>\n",
       "      <td>-1.276302</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Word  Coefficient\n",
       "242       to help    -0.872962\n",
       "262        us and    -0.881678\n",
       "4      accused of    -0.950826\n",
       "153    people are    -0.974572\n",
       "20     around the    -1.011382\n",
       "175  south africa    -1.020035\n",
       "6     after being    -1.092408\n",
       "244       to kill    -1.198554\n",
       "188   the country    -1.224351\n",
       "260         up in    -1.276302"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nvectorize = TfidfVectorizer(min_df=0.05, max_df=0.85,ngram_range=(2,2)) # DEFINING THE TFID TRANSFORMATION FUNCTION\n",
    "news_nvector = nvectorize.fit_transform(train_news_list)\n",
    "\n",
    "print(\" TFID TRANSFOMATION DATAFRAME SHAPE\",news_nvector.shape)\n",
    "\n",
    "\n",
    "\n",
    "test_news_list = []\n",
    "for row in range(0,len(test_news.index)):\n",
    "    test_news_list.append(' '.join(str(x) for x in test_news.iloc[row,2:27])) # CONVERT THE TESTING DATASET OF 27 COLUMNS INTO ONE ELEMENT IN THE LIST FOR EACH DAY\n",
    "ntest_vector = nvectorize.transform(test_news_list)\n",
    "npredictions = nmodel.predict(ntest_vector)\n",
    "\n",
    "pd.crosstab(test_news[\"Label\"], npredictions, rownames=[\"Actual\"], colnames=[\"Predicted\"])\n",
    "\n",
    "accuracy2=accuracy_score(test_news['Label'], npredictions)\n",
    "print(\" Logistics Regression with Bigram and TFID\",accuracy2)\n",
    "\n",
    "nwords = nvectorize.get_feature_names_out()\n",
    "ncoefficients = nmodel.coef_.tolist()[0]\n",
    "ncoeffdf = pd.DataFrame({'Word' : nwords, \n",
    "                        'Coefficient' : ncoefficients})\n",
    "ncoeffdf = ncoeffdf.sort_values(['Coefficient', 'Word'], ascending=[0, 1])\n",
    "ncoeffdf.head(10)\n",
    "ncoeffdf.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd751be1-03c3-4c8a-8ef8-26da4d96cb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest with tfid and bigram 0.5372233400402414\n"
     ]
    }
   ],
   "source": [
    "nvectorize = TfidfVectorizer(min_df=0.01, max_df=0.95,ngram_range=(2,2))\n",
    "news_nvector = nvectorize.fit_transform(train_news_list)\n",
    "\n",
    "rfmodel = RandomForestClassifier(random_state=55)  #DEFINNG THE RANDOM FOREST MODEL\n",
    "rfmodel = rfmodel.fit(news_nvector, train_news[\"Label\"])\n",
    "test_news_list = []\n",
    "for row in range(0,len(test_news.index)):\n",
    "    test_news_list.append(' '.join(str(x) for x in test_news.iloc[row,2:27]))\n",
    "ntest_vector = nvectorize.transform(test_news_list)\n",
    "\n",
    "rfpredictions = rfmodel.predict(ntest_vector)\n",
    "accuracyrf = accuracy_score(test_news['Label'], rfpredictions)\n",
    "print(\"Random forest with tfid and bigram\", accuracyrf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0060e436-76b4-431c-ac89-fc7be43aec5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes accuracy:  0.5291750503018109\n"
     ]
    }
   ],
   "source": [
    "nvectorize = TfidfVectorizer(min_df=0.05, max_df=0.8,ngram_range=(2,2)) #DEFINING THE NAIVE BAYS MODEL\n",
    "news_nvector = nvectorize.fit_transform(train_news_list)\n",
    "\n",
    "nbmodel = MultinomialNB(alpha=0.5)\n",
    "nbmodel = nbmodel.fit(news_nvector, train_news[\"Label\"])\n",
    "\n",
    "test_news_list = []\n",
    "for row in range(0,len(test_news.index)):\n",
    "    test_news_list.append(' '.join(str(x) for x in test_news.iloc[row,2:27])) # CONVERT THE TESTING DATASET OF 27 COLUMNS INTO ONE ELEMENT IN THE LIST FOR EACH DAY\n",
    "ntest_vector = nvectorize.transform(test_news_list)\n",
    "\n",
    "nbpredictions = nbmodel.predict(ntest_vector)\n",
    "nbaccuracy=accuracy_score(test_news['Label'], nbpredictions)\n",
    "print(\"Naive Bayes accuracy: \",nbaccuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7d9e9270-4797-40f0-b20e-621692f7e08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " CONFUSION MATRIX OF THE GRADIANT BOOSTING  [[ 92 148]\n",
      " [ 74 183]]\n",
      "Gradient Boosting accuracy:  0.5533199195171026\n"
     ]
    }
   ],
   "source": [
    "gbmodel = GradientBoostingClassifier(random_state=52)  # DEFINING THE GARDIANT BOOSTING MODEL\n",
    "gbmodel = gbmodel.fit(news_nvector, train_news[\"Label\"])\n",
    "test_news_list = []\n",
    "for row in range(0,len(test_news.index)):\n",
    "    test_news_list.append(' '.join(str(x) for x in test_news.iloc[row,2:27]))\n",
    "ntest_vector = nvectorize.transform(test_news_list)\n",
    "\n",
    "gbpredictions = gbmodel.predict(ntest_vector.toarray())\n",
    "gbaccuracy = accuracy_score(test_news['Label'], gbpredictions)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(\" CONFUSION MATRIX OF THE GRADIANT BOOSTING \", confusion_matrix(test_news['Label'], gbpredictions))\n",
    "\n",
    "\n",
    "print(\"Gradient Boosting accuracy: \",gbaccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "16621adf-1f08-4010-ba40-ccc6b58c9ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1492, 569061)\n",
      "TRIGARAM ACCURACY 0.5171026156941649\n",
      "trigram top ten word distibution                       Word  Coefficient\n",
      "509383           to the us     0.202613\n",
      "481307        the right to     0.172469\n",
      "322285   nobel peace prize     0.168217\n",
      "223934  human rights watch     0.161099\n",
      "491158         this is the     0.155940\n",
      "240342        in west bank     0.152050\n",
      "230935        in china the     0.139418\n",
      "518984         turn out to     0.138432\n",
      "239146     in the occupied     0.132997\n",
      "321584         no fly zone     0.127843\n",
      "trigram last ten word distibution                          Word  Coefficient\n",
      "183898      freedom of speech    -0.141942\n",
      "356524        osama bin laden    -0.144730\n",
      "371338  phone hacking scandal    -0.148406\n",
      "497344              to be the    -0.148879\n",
      "207018      has been arrested    -0.152867\n",
      "509742              to try to    -0.153109\n",
      "334728        of human rights    -0.172338\n",
      "416292             said to be    -0.192564\n",
      "48303        around the world    -0.197741\n",
      "238814         in the country    -0.225204\n"
     ]
    }
   ],
   "source": [
    "n3vectorize = TfidfVectorizer(min_df=0.0004, max_df=0.115,ngram_range=(3,3)) # DEFINING THE TFID , TRIGRAM MODEL\n",
    "news_n3vector = n3vectorize.fit_transform(train_news_list)\n",
    "\n",
    "print(news_n3vector.shape)\n",
    "\n",
    "n3model = lr.fit(news_n3vector, train_news[\"Label\"])\n",
    "\n",
    "test_news_list = []\n",
    "for row in range(0,len(test_news.index)):\n",
    "    test_news_list.append(' '.join(str(x) for x in test_news.iloc[row,2:27])) # CONVERT THE TESTING DATASET OF 27 COLUMNS INTO ONE ELEMENT IN THE LIST FOR EACH DAY\n",
    "n3test_vector = n3vectorize.transform(test_news_list)\n",
    "n3predictions = n3model.predict(n3test_vector)\n",
    "\n",
    "pd.crosstab(test_news[\"Label\"], n3predictions, rownames=[\"Actual\"], colnames=[\"Predicted\"])\n",
    "\n",
    "accuracy3=accuracy_score(test_news['Label'], n3predictions)\n",
    "print(\"TRIGARAM ACCURACY\", accuracy3)\n",
    "\n",
    "n3words = n3vectorize.get_feature_names_out()\n",
    "n3coefficients = n3model.coef_.tolist()[0]\n",
    "n3coeffdf = pd.DataFrame({'Word' : n3words, \n",
    "                        'Coefficient' : n3coefficients})\n",
    "n3coeffdf = n3coeffdf.sort_values(['Coefficient', 'Word'], ascending=[0, 1])\n",
    "print(\"trigram top ten word distibution\", n3coeffdf.head(10))\n",
    "print(\"trigram last ten word distibution\", n3coeffdf.tail(10))    # trigram model word distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "94555c41-b73d-41c6-920a-fa682e9e0d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[101 139]\n",
      " [113 144]]\n",
      "Sentiment Accuracy 0.49295774647887325\n",
      "f1_score__ 0.4906454882008645\n"
     ]
    }
   ],
   "source": [
    "train_sentiment=train_news\n",
    "test_sentiment = test_news\n",
    "train_sentiment =train_sentiment.drop(['Date', 'Label'], axis=1)\n",
    "for column in train_sentiment:\n",
    "    train_sentiment[column]=train_sentiment[column].apply(analize_sentiment)  #converting the train headlines into polarity scores\n",
    "train_sentiment = train_sentiment+10  # removing negative co:efficient from the datset for better performance\n",
    "\n",
    "test_sentiment =test_sentiment.drop(['Date', 'Label'], axis=1)\n",
    "for column in test_sentiment:\n",
    "    test_sentiment[column]=test_sentiment[column].apply(analize_sentiment) # converting the test headlines into ploarity \n",
    "test_sentiment=test_sentiment+10 # removing negative co:efficient from the datset for better performance\n",
    "\n",
    "XGB_model= XGBClassifier()  # training the polarity score datset with DIJA \n",
    "gradiant=XGB_model.fit(train_sentiment, train_news['Label'])\n",
    "y_pred= gradiant.predict(test_sentiment)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(test_news['Label'], y_pred))\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Sentiment Accuracy\",accuracy_score(test_news['Label'], y_pred))\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"f1_score__\",f1_score(test_news['Label'], y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193e1d8b-5de8-4bb1-a3dc-2a8730781f2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
